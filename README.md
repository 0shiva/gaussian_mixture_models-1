Gaussian Mixture Models (GMM) & Expectation–Maximization (EM) Algorithm  
----

If one Gaussian is awesome, what would be more awesome? __Two Gaussians__

That is what Gaussian Mixture Models (GMM) do, take what you know and love, the Gaussian, and mix several of them together. 

How does that dark magic happen? The Expectation–Maximization (EM) Algorithm.




----
__Suggested Preparation Materials__:

- Listen [EM on Talking Machines podcast](http://www.thetalkingmachines.com/blog/2015/10/9/machine-learning-mastery-and-cancer-clusters)
- Watch GMM & EM series, videos 16.3-16.13 from [mathematicalmonk](https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA8)
- "What is the expectation maximization algorithm?" pdf in readings folder

__Challenge Preparation Materials__:

- "EM Demystified- An Expectation-Maximization Tutorial" in readings folder
- [Theoretical Statistical approach to EM](http://www.stat.ucla.edu/~dinov/courses_students.dir/04/Spring/Stat233.dir/STAT233_notes.dir/EM_Tutorial)
